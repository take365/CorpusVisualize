# Hierarchical Multi-Modal Diarization (HMD)

**Version:** 0.1
**Author:** take365
**License:** MIT
**Category:** Speech Understanding / Multi-Modal Dialogue Analysis

---

## ðŸ§­ Overview

**Hierarchical Multi-Modal Diarization (HMD)** ã¯ã€  ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ã®äºŒã¤ã®æƒ…å ±æºã‚’éšŽå±¤çš„ã«çµ±åˆã—ã€
ã‚ˆã‚Šè‡ªç„¶ãªè©±è€…è­˜åˆ¥ãƒ»å¯¾è©±æ§‹é€ ç†è§£ã‚’å®Ÿç¾ã™ã‚‹æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

å¾“æ¥ã®éŸ³éŸ³ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¸­å¿ƒã®è©±è€…åˆ†é›¢ã¯ã€æ–‡è„ˆç†è§£ãŒãªãã€LLM ãƒ™ãƒ¼ã‚¹ã®è¨€èªžçš„è©±è€…æŽ¨å®šã¯éŸ³å£°ç‰¹å¾´ã‚’ç„¡è¦–ã—ã¦ã„ã¾ã—ãŸã€‚

HMD ã¯ãã®ä¸¡è€…ã‚’çµ±åˆã—ã€
**ã€Œæ–‡è„ˆçš„ã«èª°ãŒè©±ã—ã¦ã„ã‚‹ã‹ã€Ã—ã€ŒéŸ³éŸ³çš„ã«èª°ã®å£°ã‹ã€**
ã‚’åŒæ™‚ã«æŽ¨å®šã—ã¾ã™ã€‚

---

## ðŸ§ System Architecture

```
 Audio Input â”€â”€â–¶ ASR (WhisperX) â”€â–¶ Stage 1
                                         â”‚
                                         â–¼
                              Text-Level Structural Analysis (LLM)
                                         â”‚
                                         â–¼
                           Semantic Segmentation + Time Alignment
                                         â”‚
                                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                              â–¼
     Text-based Speaker Inference      Audio-based Speaker Embedding
        (LLM contextual inference)        (pyannote / ECAPA / x-vector)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                     Speaker Fusion Engine
                             â–¼
                     Final Diarization Map
```

---

## ðŸ§© Stage Definitions

### **Stage 1 â€“ ASR + Temporal Anchoring**

* **Input:** WAV éŸ³å£°
* **Process:** WhisperX ã«ã‚ˆã‚‹é«˜ç²¾åº¦æ–‡å­—èµ·ã“ã—ï¼‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
* **Output:** `segments[{start, end, text}]`

---

### **Stage 2 â€“ Discourse Structure Recognition (LLM)**

* **Goal:** ç™ºè¨€æ§‹é€ ã‚’ç†è§£ã—ã€
  ï¼ˆ1ï¼‰ã²ã¨ã‚Šèªžã‚Šï¼ï¼ˆ2ï¼‰å¯¾è©±ï¼ï¼ˆ3ï¼‰è¤‡æ•°äººã®è«–è­°ã‚’åˆ¤å®šã€‚
* **Method:** LLM ã«å…¨æ–‡ã‚’å…¥åŠ›ã—ã€è©±è€…æ•°ãƒ»é–¢ä¿‚æ€§ã‚’æŽ¨å®šã€‚
* **Output:** JSON `{n_speakers, roles, relation_matrix}`

---

### **Stage 3 â€“ Semantic Segmentation & Sentence Reconstruction**

* **Goal:** æ–‡æ„å˜ä½ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å†æ§‹æˆã—ã€éŸ³å£°æ™‚åˆ»ã¨çµåˆã€‚
* **Method:** LLM ã«ã‚ˆã‚Šæ–‡å¢ƒã‹ã‚‰æ–‡å¢ƒå¢ƒç•Œã‚’æŽ¨å®šã€‚
* **Output:** `sentences[{start, end, text, context}]`

---

### **Stage 4 â€“ Dual-Path Speaker Attribution**

* **Goal:** å„æ–‡ã«å¯¾ã—ã¦è©±è€…ã‚’éŸ³å£°ãƒ»æ–‡è„ˆã®ä¸¡é¢ã‹ã‚‰æŽ¨å®šã€‚

* **Submodules:**

  * **(a) Text-based Inference:** LLM ãŒå¯¾è©±æ–‡è„ˆã‹ã‚‰è©±è€…ã‚’æŽ¨å®šã€‚
  * **(b) Audio-based Clustering:** ECAPA / pyannote ã«ã‚ˆã‚‹å£°ç­‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€‚
  * **(c) Fusion:** æ–‡è„ˆç¢ºä¿¡åº¦ã¨å£°ç­‰è·é›¢ã®ç©åˆ (Weighted Voting / Bayesian Fusion)ã€‚

* **Output:**  `[{start, end, text, speaker_id, speaker_conf_text, speaker_conf_audio}]`

---

## ðŸ§® Output Format

```json
[
  {
    "start": 0.031,
    "end": 27.065,
    "text": "å…ˆç”Ÿã€ã“ã‚“ã«ã¡ã¯ã€‚ãƒ†ã‚¹ãƒˆè‰¯ã‹ã£ãŸã®ã‹ãªï¼Ÿ",
    "speaker_text": "Teacher",
    "speaker_audio": "spk_01",
    "speaker_fused": "A",
    "confidence": {
      "textual": 0.87,
      "acoustic": 0.78,
      "fusion": 0.84
    }
  }
]
```

---

## ðŸ“Š Evaluation Pipeline

| Step | Script                  | Purpose                      |
| ---- | ----------------------- | ---------------------------- |
| 1    | `run_whisperx_batch.py` | ASR + diarization (WhisperX) |
| 2    | `run_hmd_stage2.py`     | LLMæ§‹é€ è§£æž (Stage 2â€“3)          |
| 3    | `run_hmd_fusion.py`     | å£°ç­‰ãƒ»æ–‡è„ˆçµ±åˆ (Stage 4)            |
| 4    | `evaluate_char.py`      | æ­£èª¤æ¯”è¼ƒ (WER)                   |
| 5    | `report_html.py`        | HTML å¯è¦–åŒ–                     |

---

## ðŸ§  Research Novelty

| è¦ç´     | å¾“æ¥æ³•       | HMD              |
| ----- | --------- | ---------------- |
| è©±è€…æ•°æ±ºå®š | äº‹å‰å›ºå®š      | LLMæ–‡è„ˆæŽ¨å®š          |
| æ–‡å˜ä½åˆ¤å®š | éŸ³å£°åˆ†å‰²      | æ„å‘³+æ™‚é–“æƒ…å ±å†æ§‹æˆ       |
| è©±è€…è­˜åˆ¥  | éŸ³éŸ³ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° | LLM+Voice Fusion |
| æŽ¨è«–ç²’åº¦  | ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½    | æ–‡å˜ä½              |
| å¿œç”¨ç¯„å›²  | ä¼šè­°è¨˜éŒ²      | å­¦ç¿’ã€è¬›ç¾©ã€åŒ»ç™‚é¢è«‡       |

---

## ðŸ§¬ Core Technical Concepts

* **Hierarchical Reasoning:** LLMãŒå¯¾è©±å…¨ä½“ã®æ§‹é€ ã‚’ç†è§£ã—ã€ä¸‹å±¤ã®åˆ¤å®šã«åæ˜ ã€‚
* **Multi-Modal Fusion:** TextåŸºç›¤ã¨VoiceåŸºç›¤ã‚’çµ±åˆã€‚
* **Explainable Attribution:** å„æ–‡ã«ç†ç”±ã¨ç¢ºä¿¡åº¦ã‚’ä»˜ä¸Žã—ã€è©±è€…è­˜åˆ¥ã‚’é€æ˜ŽåŒ–ã€‚

---

## ðŸ”¬ Planned Extensions

* âœ… OpenRouter LLM APIè‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
* âœ… ECAPA-TDNN / Whisper speaker embedding æ¯”è¼ƒ
* âŸ³ Online diarization (real-time adaptation)
* ðŸ” Japanese Dialogue datasets (JVS, CSJ) fine-tuning

---

## ðŸ“˜ References

* Wang et al., *DiarizationLM: Large Language Models for Speaker Attribution*, Google Research, 2024
* Paturi et al., *AG-LSEC: Adaptive Guided Lexical Speaker Error Correction*, Amazon Alexa, 2024
* Zhu et al., *SpeakerLM: Multi-Modal Speaker Understanding via LLMs*, HKUST, 2025
* Lin et al., *Diarization-Aware Multi-Stream ASR*, 2025

---

## ðŸ§¹ Repository Structure (planned)

```
research/hmd/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ hmd_stage1_asr.py
â”‚   â”œâ”€â”€ hmd_stage2_structure.py
â”‚   â”œâ”€â”€ hmd_stage3_segment.py
â”‚   â”œâ”€â”€ hmd_stage4_fusion.py
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ data/
â”œâ”€â”€ runs/
â””â”€â”€ README.md
```

---

## ðŸ’¬ Motto

> "Not just who is speaking â€” but **why and how we know** who is speaking."
> â€” *HMD: Hierarchical Multi-Modal Diarization, take365 (2025)*
