# 話者分離 試行録

ざっくりいうと、最終的には ASR→ECAPA 埋め込み→PCA→KMeans というシンプルな流れに落ち着きました。
その前には WhisperX や faster-whisper での分割違和感を確かめたり、クラスタリング結果をダッシュボードで比較したりもしています。
ついでにクラウド API も試しましたが日本語の話者分離はまだ厳しく、自前検証の積み重ねに回帰した形です。

## 1. `speaker_diarization` サンドボックス期

- Energy ベースと pyannote.audio ベースを比較し、正解率が Energy 55〜71%、pyannote 35〜48% と判明した (`research/speaker_diarization/report/pre_llm_summary.md:4`)。
- 同時に Google Cloud Speech-to-Text v2 を diarization 付きで呼び出したが、日本語モデルでは機能未対応でエラーとなり、英語モデルを流用しても実質失敗だった (`research/speaker_diarization/report/pre_llm_summary.md:10`)。この検証用に `run_gcloud_stt_batch.py` を書き起こして REST/batch の両モードを実装している (`research/speaker_diarization/src/run_gcloud_stt_batch.py:1`)。
- WhisperX まわりでは `run_whisperx_batch.py` でモデル・VAD 閾値・デバイスを切り替えながら一括実行＆評価できるようにし (`research/speaker_diarization/src/run_whisperx_batch.py:1`)、`runs/whisperx/2025-10-22_*_vad0xx` や `report/whisperx/large_v2_cpu_int8/…` といったディレクトリに各試行の生出力と指標を残した。

## 2. HMD での多特徴クラスタリング構想

- HMD では文単位クラスタリングの要件を `クラスタリングの可視化.txt` に整理し、話者埋め込み・プロソディ・音響統計・スペクトル・環境の 5 系列を横並びで可視化する方針を決めた (`research/HMD/クラスタリングの可視化.txt:9`)。
- その仕様を具現化したのが `hmd_sentence_cluster.py` で、WhisperX（または同等の `segments.jsonl`）と LLM 予測を入力に、各特徴で PCA/UMAP→DBSCAN→Plotly ダッシュボードまで作り込んでいる (`research/HMD/src/hmd_sentence_cluster.py:1`)。
- 実験結果は `research/HMD/data/試行錯誤/<dialogue>/` にフルセットで残っており、特徴 JSONL・DBSCAN ラベル・Plotly HTML を後から比較できる状態になっている（例: `research/HMD/data/試行錯誤/LD01-Dialogue-01/viz/index.html`）。

## 3. パラメータ探索の自動化

- 多層パイプラインの検証を楽にするため、スピーカー埋め込みに絞って前処理と DBSCAN パラメータをスイープし、ギャップ統計やシルエットを HTML にまとめる `hmd_cluster_sweep.py` を追加した (`research/HMD/src/hmd_cluster_sweep.py:1`)。
- `cluster_sweep/` 配下には raw/zscore など前処理ごとのセクションが並び、eps/min_samples の最適値や外れ率を可視化するレポートが残る（例: `research/HMD/data/試行錯誤/LD01-Dialogue-01/cluster_sweep/index.html`）。

## 4. Whisper セグメントの練り直し

- WhisperX 由来の区切りが文境界と噛み合わないケースが多かったため、faster-whisper で自前 1shot のセグメント化を試す `transcribe_whisper.py` を導入し、VAD ウィンドウや `condition_on_previous_text=False` を細かく指定した (`research/HMD/src/transcribe_whisper.py:101`)。
- large-v2 モデルで回した初期データは `/試行錯誤/*/meta.json` に残っており、発話冒頭に話者名をハルシネーションしがちだった (`research/HMD/data/試行錯誤/LD01-Dialogue-01/meta.json:3`)。そのため初期プロンプトで「話者名を付けない」よう明示し (`research/HMD/src/transcribe_whisper.py:116`)、モデルも large-v3-turbo へ切り替えて安定度を上げている (`research/HMD/data/LD01-Dialogue-01/meta.json:3`)。
- OpenAI 純正を使いたいケース向けには `transcribe_whisper_pure.py` もあり、segments/words の互換 JSONL を生成して quick cluster 系と併用できるようにした (`research/HMD/src/transcribe_whisper_pure.py:1`)。

## 5. `hmd_quick_cluster.py` による現行フロー

- 試行錯誤の結果、ASR→ECAPA 埋め込み→PCA→KMeans→成果物保存を 1 本で完結させる `hmd_quick_cluster.py` に収束 (`research/HMD/src/hmd_quick_cluster.py:5`)。
- CLI/.env/デフォルトの優先順位や Whisper の再利用設定を整えつつ、PCA(≤50) で評価空間を確保し、シルエット最大の k を選択または固定 k を使う構成になっている (`research/HMD/src/hmd_quick_cluster.py:336`)。
- 出力は `features/speaker_embedding.jsonl`、`embeddings/pca2.jsonl`、`clusters/kmeans_pca2.json`、`viz/quick_scatter.html` のセットで、従来パイプラインとも互換なディレクトリレイアウトを保っている (`research/HMD/src/hmd_quick_cluster.py:577`)。

## 6. 現状の成果物

- 最新データセット（例: `research/HMD/data/LD01-Dialogue-01/`）には large-v3-turbo で再生成したセグメント、quick cluster のクラスタ結果、可視化 HTML が揃っており、旧 `試行錯誤` ディレクトリと並べて比較することで改善点を追跡できる。
- `speaker_diarization` 側のレポート（`report/pre_llm_summary.md` や `report/whisperx/...`）と突き合わせることで、ASR/diarization 選定の履歴を後から参照できる状態を維持している。
