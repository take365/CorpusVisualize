\# VoTex Visualize — DETAILS \& TASKS (for Codex CLI)



Codex CLI にそのまま投げ込んで動かすことを想定した \*\*設計・仕様・タスク\*\*。最小実装→配線→可視化までの順で分解。



---



\## 0) リポジトリ構成（提案）



```

voTex/

&nbsp; ├─ pipeline/

&nbsp; │   ├─ diarization.py        # 話者分離

&nbsp; │   ├─ asr.py                # ASR(WhisperX)

&nbsp; │   ├─ features/

&nbsp; │   │    ├─ emotion.py       # SER: wav2vec2/くしなだ/Fusion

&nbsp; │   │    ├─ pitch.py         # Praat(pyParselmouth)/pyWORLD/J-ToBIルール

&nbsp; │   │    ├─ loudness.py      # RMS/LUFS/A-weighted

&nbsp; │   │    ├─ tempo.py         # chars/sec / syllable-rate / VAD ratio

&nbsp; │   │    ├─ dialect.py       # TF-IDF / LLM+RAG / prosody

&nbsp; │   │    └─ lexicon.py       # 品詞/品位/Valence

&nbsp; │   ├─ aggregate.py          # 話者集約

&nbsp; │   ├─ export.py             # JSONL/Parquet出力

&nbsp; │   └─ run\_pipeline.py       # CLIエントリ

&nbsp; ├─ ui/

&nbsp; │   └─ VoTexConversationViz.tsx  # Reactサンプル（既存）

&nbsp; ├─ data/

&nbsp; │   ├─ audio/                # 入力音声

&nbsp; │   └─ processed/            # 出力

&nbsp; ├─ models/                   # 事前学習モデル格納(任意)

&nbsp; ├─ configs/

&nbsp; │   └─ default.yaml          # 抽出手法/閾値の設定

&nbsp; ├─ tests/

&nbsp; ├─ requirements.txt or pyproject.toml

&nbsp; └─ README.md

```



---



\## 1) セグメント JSON 契約（UI/バッチ共通）



```json

{

&nbsp; "id": "s03",

&nbsp; "start": 12.0,

&nbsp; "end": 21.4,

&nbsp; "speaker": "A",

&nbsp; "text": "なるほど、それは面白いですね。",

&nbsp; "emotion": {"joy":0.22, "anger":0.04, "sad":0.06, "neutral":0.68},

&nbsp; "pitch": \[205,212,219,225],

&nbsp; "loudness": 0.63,

&nbsp; "tempo": 4.2,

&nbsp; "dialect": {"kansai":0.51, "kanto":0.22, "tohoku":0.12, "kyushu":0.10, "hokkaido":0.05},

&nbsp; "highlights": \[{"startChar":0, "endChar":6, "tag":"dialect"}]

}

```



\* 1ファイルに複数セグメントを \*\*JSONL\*\*（1行=1セグメント）で保存。

\* 話者集約統計は `speakers.parquet` に保存。



---



\## 2) CLI 仕様（`pipeline/run\_pipeline.py`）



```bash

python pipeline/run\_pipeline.py \\

&nbsp; --in data/audio \\

&nbsp; --out data/processed \\

&nbsp; --emotion ser\_w2v \\

&nbsp; --pitch praat \\

&nbsp; --loudness lufs \\

&nbsp; --tempo asr\_chars \\

&nbsp; --dialect lexicon\_tfidf \\

&nbsp; --lexicon pos\_dict \\

&nbsp; --asr whisperx \\

&nbsp; --diar pyannote \\

&nbsp; --num\_workers 4 \\

&nbsp; --min\_seg\_sec 2.0 --max\_seg\_sec 30.0

```



\*\*動作\*\*：



1\. 音声読み込み → 2) 話者分離 → 3) ASR → 4) 特徴抽出（各方式はフラグで切替） → 5) 集約・書き出し



---



\## 3) 実装タスク（チェックリスト）



\### A. 基盤セットアップ



\* \[ ] `requirements.txt`（ffmpeg, torchaudio, praat-parselmouth, pyworld, pyannote, faster-whisper, transformers, pandas, numpy, pydantic, rich, typer など）

\* \[ ] `Makefile` 用意（`make setup`, `make test`, `make run`）

\* \[ ] `configs/default.yaml`（方式と閾値の既定値）



\### B. 話者分離 \& ASR



\* \[ ] `diarization.py`：pyannote で (start, end, speaker) 取得

\* \[ ] `asr.py`：WhisperX で speaker-attributed transcript 生成

\* \[ ] 両者を結合して \*\*初期 segments.jsonl\*\* 出力



\### C. 特徴抽出（モジュールごとに2–3方式）



\* \[ ] `emotion.py`：`ser\_w2v` / `ser\_kushinada` / `ser\_fusion`（ダミー→本実装）

\* \[ ] `pitch.py`：`praat`(F0) / `pyworld` / `jtobi\_rule`（簡易アクセント型）

\* \[ ] `loudness.py`：`rms` / `lufs` / `a\_weighted`

\* \[ ] `tempo.py`：`asr\_chars` / `syllable\_rate` / `vad\_ratio`

\* \[ ] `dialect.py`：`lexicon\_tfidf` / `llm\_rag` / `audio\_prosody`

\* \[ ] `lexicon.py`：`pos\_dict` / `formality` / `valence`

\* \[ ] `aggregate.py`：話者単位の統計作成

\* \[ ] `export.py`：JSONL/Parquet 書き出し



\### D. CLI 配線 \& ログ



\* \[ ] `run\_pipeline.py`：Typer/Click で CLI 実装、方式フラグで分岐

\* \[ ] 進捗バー（rich.progress）、処理ログ（JSONL）

\* \[ ] 例外処理（欠損データ・空音声・ASR失敗のスキップ）



\### E. UI 連携



\* \[ ] `segments.jsonl` のインポート機能（React 側）

\* \[ ] 抽出方式の選択状態を UI で表示（右上セレクタに反映済）

\* \[ ] グラフ更新／ツールチップ（根拠表示）



\### F. テスト \& 検証



\* \[ ] `tests/test\_schema.py`：セグメントJSONのスキーマ検証（pydantic）

\* \[ ] `tests/test\_extractors.py`：各モジュールの最小ケース

\* \[ ] `tests/test\_cli.py`：CLIの入出力統合テスト

\* \[ ] サンプル音声3本で E2E（処理→UI表示）



---



\## 4) 受け入れ条件（Acceptance Criteria）



\* \[ ] `make run` で `data/audio/\*.wav` を処理し、`data/processed/segments.jsonl` と `speakers.parquet` が生成される

\* \[ ] `segments.jsonl` を UI で読み込むと、タイムライン・グラフ・ハイライトが表示される

\* \[ ] 抽出方式フラグを変えると、出力に方式名が記録される（再現性のため）

\* \[ ] 欠損（ASR失敗、無音など）があってもパイプラインが止まらずスキップ

\* \[ ] 単体・統合テストが CI で通る



---



\## 5) Codex CLI への投げ方（例）



\*\*プロンプト雛形\*\*



```

\# Goal

pipeline/run\_pipeline.py を Typer で実装。上記 CLI 仕様に沿って、ダミー実装でも良いので end-to-end で JSONL を出力できる形にする。



\# Constraints

\- pydantic で SegmentSchema を定義

\- 方式フラグを --emotion/--pitch/--loudness/--tempo/--dialect/--lexicon で受け取る

\- "ダミーの抽出値" を返す helper を実装し、後で本実装と差し替えできる構造



\# Files to create

\- pipeline/run\_pipeline.py

\- pipeline/features/emotion.py (dummy)

\- pipeline/features/pitch.py (dummy)

\- pipeline/features/loudness.py (dummy)

\- pipeline/features/tempo.py (dummy)

\- pipeline/features/dialect.py (dummy)

\- pipeline/features/lexicon.py (dummy)

\- pipeline/aggregate.py (dummy)

\- pipeline/export.py (jsonl writer)

\- tests/test\_schema.py

```



---



\## 6) 設定ファイル（configs/default.yaml の雛形）



```yaml

asr: whisperx

&nbsp;diarization: pyannote

&nbsp;emotion: ser\_w2v

&nbsp;pitch: praat

&nbsp;loudness: lufs

&nbsp;tempo: asr\_chars

&nbsp;dialect: lexicon\_tfidf

&nbsp;lexicon: pos\_dict

&nbsp;min\_seg\_sec: 2.0

&nbsp;max\_seg\_sec: 30.0

&nbsp;sample\_rate: 16000

&nbsp;output\_format: jsonl

```



---



\## 7) 最小テストデータ



\* `data/audio/sample\_01.wav` （10–30秒、2話者っぽい）

\* 期待：`segments.jsonl` に最低 3 セグメント以上、各セグメントに `emotion/pitch/loudness/tempo/dialect` キーが存在



---



\## 8) セキュリティ / 倫理メモ



\* PII（人名・電話・住所）は ASR 直後に正規表現でマスク（`export.py` でオプション化）

\* ログに原文テキストを出さないモード（`--no-text`）

\* 方式・閾値はすべて出力に記録（監査用）



---



これをベースに、\*\*まずはダミー抽出で E2E\*\* → 次に実抽出器を差し替え、精度＆速度を上げてください。



