# CorpusVisualize パイプライン解説

## 1. やさしい解説（難しい言葉を使わない）

CorpusVisualize は、会話の音声を「見える化」するための仕組みです。流れはとてもシンプル。

1. **音声を用意する** – フォルダに WAV などの音声ファイルを入れます。
2. **パイプラインを動かす** – `pipeline/run_pipeline.py` を実行すると、音声ごとに話している部分を切り出し、文字起こしをします。
3. **特徴をまとめる** – 感情や話す速さなどをざっくり計算して、ひとつの JSONL ファイル（`segments.jsonl`）にまとめます。話者ごとの集計は Parquet ファイル（`speakers.parquet`）に入ります。
4. **UI で見る** – ブラウザの UI（`ui/` ディレクトリ）に `segments.jsonl` を読み込ませれば、会話タイムラインやグラフが表示されます。マイク録音機能を使えば、その場で話した内容をすぐに可視化することもできます。

要するに「音声 → 文字と数字 → グラフ表示」という流れをワンストップで提供する仕組みです。難しいサーバ設定は不要で、ファイルを用意してコマンドを一つ実行するだけで可視化用データが手に入ります。

---

## 2. 技術的な解説

### 2.1 パイプライン構成

```
CorpusVisualize/
├─ pipeline/
│   ├─ run_pipeline.py        # Typer CLI エントリ
│   ├─ audio.py               # librosa ベースの音声読み込み
│   ├─ diarization.py         # エネルギー変化で区間を区切る簡易話者分離
│   ├─ asr.py                 # Whisper (faster-whisper) / Web Speech ASR
│   ├─ features/
│   │    ├─ emotion.py        # 簡易感情スコア
│   │    ├─ pitch.py          # librosa.yin によるピッチ推定
│   │    ├─ loudness.py       # RMS から正規化
│   │    ├─ tempo.py          # テキスト長や音声から速度推定
│   │    ├─ dialect.py        # キーワードベースの方言スコア
│   │    └─ lexicon.py        # 語彙ハイライト（辞書）
│   ├─ aggregate.py           # 話者別統計の計算
│   ├─ export.py              # JSONL / Parquet 出力
│   └─ types.py               # Pydantic モデル定義
├─ ui/                        # React + Tailwind + Recharts による可視化 UI
└─ docs/                      # 仕様・資料
```

### 2.2 処理フロー

1. **音声読み込み** – `librosa.load` で 16kHz モノラルに正規化。
2. **話者区間検出** – `EnergyBasedDiarizer` が短時間エネルギーを見て区切りを作り、交互にスピーカー A/B を割り当て。
3. **ASR（文字起こし）**
   - CLI では `--asr whisper-medium` を指定すると `faster-whisper` が GPU/CPU で文字起こし。
   - Web デモでは Mic タブが Web Speech API（ブラウザの `SpeechRecognition`）を利用し、最終結果単位でセグメント化。
4. **特徴抽出** – 各セグメントに対して以下を実行:
   - 感情: 音量や分散からダミー分布を生成（後で本物の感情モデルに差し替え可能）。
   - ピッチ: librosa の `yin` で F0 を配列として取得。
   - 音量: RMS を 0-1 に正規化。
   - テンポ: 文字数 / 秒数 など簡易計算。
   - 方言: キーワード一致によるスコア（Uniform fallback あり）。
   - 語彙: 辞書マッチによるハイライト。
5. **スキーマ検証** – `SegmentSchema` (Pydantic) でフォーマット保証。`tests/` 配下で単体・CLI テストを用意。
6. **集計と出力** – `aggregate_speakers` で話者別の平均値などをまとめ、`segments.jsonl` と `speakers.parquet` に保存。
* Whisper の生テキストを `conversation_id.raw.txt` として出力（`segments.jsonl` と同じフォルダ）。整形前の結果を比較する際に利用できます。

### 2.3 UI 側のポイント

- Vite + React + Tailwind + Recharts で構築。`npm run dev` (開発) / `npm run build` (本番ビルド)。
- Import タブでは JSONL / JSON を読み込み、解析済みデータをそのまま可視化。
- Mic タブでは Web Speech API を用いてブラウザ内で音声認識→簡易解析し、同じ UI に可視化。録音結果の JSON ダウンロードも可能（後から Import タブで再利用できる）。
- チャート: タイムライン, AreaChart(感情), LineChart(ピッチ), BarChart(音量/テンポ), スタックBar(方言), リスト(語彙ハイライト)。

### 2.4 Whisper 実行とカスタマイズ

- `requirements.txt` に `faster-whisper`, `librosa`, `pydantic`, `typer`, `rich`, `pandas`, `pyarrow` を含む。
- Whisper Medium は CUDA/cuDNN があれば自動で GPU (float16)、無ければ CPU (int8) にフォールバック。
- `--asr whisper-small` などモデルサイズ変更も可能。パラメータは CLI フラグから `PipelineSettings` へ渡る。

### 2.5 テスト・再現方法

```
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pytest

# Whisper を使ってパイプラインを走らせる例
python -m pipeline.run_pipeline \
  --input-dir data/STUDIES \
  --output-dir data/processed_studies \
  --asr whisper-medium

# UI
cd ui
npm install
npm run dev
```

Mic タブは HTTPS + Chrome/Edge + `SpeechRecognition` 対応ブラウザが必要。録音→解析→可視化→JSONダウンロードまでフロントエンドのみで完結します。

### 2.3 語単位の解析 (New)

* Whisper の語タイムスタンプを利用して `words` 配列（テキスト・開始/終了時刻）を出力。
* `fugashi + unidic-lite` で各語の仮名とアクセント種別（Heiban/Atamadaka 等）を取得。
* `pyworld`（または librosa）で語区間の F0 カーブと平均ピッチを計算。
* 音量（RMS）・テンポ（文字数/秒）も語ごとに算出し、Valence/Arousal（SER）を継承。
* `.env` と CLI フラグで `SER_BACKEND` / `PROSODY_BACKEND` / `WORD_PITCH_BACKEND` を切り替え可能。

UI 側は `words` 情報を使うことで、チャット表示やカラオケ風ハイライト（`docs/karaoke_chat_va.jsx`）に実測値を反映できます。
